{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "200fc037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "from copy import copy\n",
      "from scipy import stats\n",
      "from sklearn import metrics,neighbors,preprocessing\n",
      "from sklearn import tree\n",
      "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
      "from sklearn.ensemble import IsolationForest\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.feature_selection import SelectPercentile, f_classif\n",
      "from sklearn.linear_model import LinearRegression, Ridge\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.linear_model import LogisticRegression as LR\n",
      "from sklearn.linear_model import SGDClassifier\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import cohen_kappa_score\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_squared_error as mse\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import r2_score,mean_squared_error\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.metrics import recall_score, accuracy_score, precision_score,f1_score, make_scorer\n",
      "from sklearn.metrics import recall_score, accuracy_score, precision_score,f1_score, make_scorer \n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "from sklearn.model_selection import cross_val_score as cv\n",
      "from sklearn.model_selection import cross_val_score, cross_validate, cross_val_predict, LeaveOneOut, KFold, StratifiedKFold\n",
      "from sklearn.model_selection import train_test_split, KFold, LeaveOneOut,GridSearchCV,RandomizedSearchCV\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
      "from sklearn.pipeline import make_pipeline, make_union\n",
      "from sklearn.preprocessing import FunctionTransformer\n",
      "from sklearn.svm import SVC\n",
      "from time import time\n",
      "from xgboost import XGBClassifier, XGBRegressor, plot_tree, plot_importance\n",
      "from xgboost import XGBClassifier, plot_tree, plot_importance\n",
      "import copy\n",
      "import csv,sys, os, errno,os.path,io\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "import multiprocessing\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy\n",
      "import seaborn as sns\n",
      "import tqdm\n",
      "import xgboost as xgb\n"
     ]
    }
   ],
   "source": [
    "from MLimports2 import *\n",
    "exec(MLimports())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c54e3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Find the best model (hyper-parameters) for classification\n",
    "# 2. Find the best models (hyper-parameters) for regression of short term incidents and long-term incidents\n",
    "# 3. Find the best All-to-All regression model\n",
    "\n",
    "# 10-fold cross-validation\n",
    "#     1. Train on train-set and perform classification (divide incidents into two groups)\n",
    "#     2. Use best model hyper-parameters to train regression model A-A, use samples based on the predicted duration\n",
    "#     3. Use best model hyper-parameters to train regression model B-B, use samples based on the predicted duration\n",
    "#     4. Collect predictions and estimate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8da66634",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpotcla_config = {    \n",
    "    \n",
    "    'sklearn.linear_model.LogisticRegression': {\n",
    "        'penalty': [\"l1\", \"l2\"],\n",
    "        'C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.],\n",
    "        'dual': [True, False]\n",
    "    },\n",
    "    \n",
    "    'sklearn.ensemble.RandomForestClassifier': {\n",
    "        'n_estimators': [100],\n",
    "        'criterion': [\"gini\", \"entropy\"],\n",
    "        'max_features': np.arange(0.05, 1.01, 0.05),\n",
    "        'min_samples_split': range(2, 21),\n",
    "        'min_samples_leaf':  range(1, 21),\n",
    "        'bootstrap': [True, False]\n",
    "    },\n",
    "\n",
    "    'sklearn.ensemble.GradientBoostingClassifier': {\n",
    "        'n_estimators': [100],\n",
    "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],\n",
    "        'max_depth': range(1, 10),\n",
    "        'min_samples_split': range(2, 21),\n",
    "        'min_samples_leaf': range(1, 21),\n",
    "        'subsample': np.arange(0.05, 1.01, 0.05),\n",
    "        'max_features': np.arange(0.05, 1.01, 0.05)\n",
    "    },\n",
    "        'xgboost.XGBClassifier': {\n",
    "        'n_estimators': [100],\n",
    "        'max_depth': range(1, 10),\n",
    "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],\n",
    "        'subsample': np.arange(0.05, 1.01, 0.05),\n",
    "        'min_child_weight': range(1, 21),\n",
    "        'n_jobs': [1],\n",
    "        'verbosity': [0]\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "tpotreg_config = {    \n",
    "\n",
    "    'sklearn.ensemble.GradientBoostingRegressor': {\n",
    "        'n_estimators': [100],\n",
    "        'loss': [\"ls\", \"lad\", \"huber\", \"quantile\"],\n",
    "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],\n",
    "        'max_depth': range(1, 10),\n",
    "        'min_samples_split': range(2, 21),\n",
    "        'min_samples_leaf': range(1, 21),\n",
    "        'subsample': np.arange(0.05, 1.01, 0.05),\n",
    "        'max_features': np.arange(0.05, 1.01, 0.05),\n",
    "        'alpha': [0.75, 0.8, 0.85, 0.9, 0.95, 0.99]\n",
    "    },\n",
    "    \n",
    "    'sklearn.ensemble.RandomForestRegressor': {\n",
    "        'n_estimators': [100],\n",
    "        'max_features': np.arange(0.05, 1.01, 0.05),\n",
    "        'min_samples_split': range(2, 21),\n",
    "        'min_samples_leaf': range(1, 21),\n",
    "        'bootstrap': [True, False]\n",
    "    },\n",
    "\n",
    "    'sklearn.linear_model.RidgeCV': {\n",
    "    },\n",
    "\n",
    "    'xgboost.XGBRegressor': {\n",
    "        'n_estimators': [100],\n",
    "        'max_depth': range(1, 10),\n",
    "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],\n",
    "        'subsample': np.arange(0.05, 1.01, 0.05),\n",
    "        'min_child_weight': range(1, 21),\n",
    "        'n_jobs': [1],\n",
    "        'verbosity': [0],\n",
    "        'objective': ['reg:squarederror']\n",
    "    },\n",
    "}\n",
    "\n",
    "from tpot import TPOTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e4b84d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/480 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: -0.9321262121765944\n",
      "\n",
      "Generation 2 - Current best internal CV score: -0.9321262121765944\n",
      "\n",
      "Generation 3 - Current best internal CV score: -0.9302253442844158\n",
      "\n",
      "Generation 4 - Current best internal CV score: -0.9302253442844158\n",
      "\n",
      "Generation 5 - Current best internal CV score: -0.9302253442844158\n",
      "\n",
      "Best pipeline: RandomForestRegressor(input_matrix, bootstrap=True, max_features=0.2, min_samples_leaf=7, min_samples_split=7, n_estimators=100)\n"
     ]
    }
   ],
   "source": [
    "X,Y = getBFS(part=[0,-1],dataset='sf')\n",
    "X = X.values\n",
    "\n",
    "tpot = TPOTRegressor(generations=5, cv=10,template='Regressor',population_size=80, verbosity=2, n_jobs=30,random_state=42, scoring='neg_mean_squared_error',config_dict=tpotreg_config)\n",
    "tpot.fit(X,np.log1p(Y))\n",
    "# print(tpot.score(X,Y))\n",
    "bestfullreg = tpot.fitted_pipeline_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "63701c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/480 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: -0.47396751895249284\n",
      "\n",
      "Generation 2 - Current best internal CV score: -0.4738674033383584\n",
      "\n",
      "Generation 3 - Current best internal CV score: -0.47145890692244413\n",
      "\n",
      "Generation 4 - Current best internal CV score: -0.47145890692244413\n",
      "\n",
      "Generation 5 - Current best internal CV score: -0.47020932464956644\n",
      "\n",
      "Best pipeline: GradientBoostingRegressor(input_matrix, alpha=0.99, learning_rate=0.1, loss=huber, max_depth=2, max_features=0.9000000000000001, min_samples_leaf=17, min_samples_split=2, n_estimators=100, subsample=1.0)\n"
     ]
    }
   ],
   "source": [
    "X,Y = getBFS(part=[0,40],dataset='sf')\n",
    "X = X.values\n",
    "\n",
    "tpot = TPOTRegressor(generations=5, cv=10, template='Regressor',population_size=80, verbosity=2, n_jobs=30,random_state=42, scoring='neg_mean_squared_error',config_dict=tpotreg_config)\n",
    "tpot.fit(X,np.log1p(Y))\n",
    "# print(tpot.score(X,Y))\n",
    "bestAreg = tpot.fitted_pipeline_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "21f82fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/480 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: -0.266778491973949\n",
      "\n",
      "Generation 2 - Current best internal CV score: -0.2660774156941762\n",
      "\n",
      "Generation 3 - Current best internal CV score: -0.2660774156941762\n",
      "\n",
      "Generation 4 - Current best internal CV score: -0.26565909575672053\n",
      "\n",
      "Generation 5 - Current best internal CV score: -0.26565909575672053\n",
      "\n",
      "Best pipeline: RandomForestRegressor(input_matrix, bootstrap=True, max_features=0.15000000000000002, min_samples_leaf=3, min_samples_split=9, n_estimators=100)\n"
     ]
    }
   ],
   "source": [
    "X,Y = getBFS(part=[40,-1],dataset='sf')\n",
    "X = X.values\n",
    "\n",
    "tpot = TPOTRegressor(generations=5, cv=10, template='Regressor',population_size=80, verbosity=2, n_jobs=30,random_state=42, scoring='neg_mean_squared_error',config_dict=tpotreg_config)\n",
    "tpot.fit(X,np.log1p(Y))\n",
    "# print(tpot.score(X,Y))\n",
    "bestBreg = tpot.fitted_pipeline_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7a17d79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/600 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.48959490677407064\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.49069298005032047\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.49069298005032047\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.49069298005032047\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.49069298005032047\n",
      "\n",
      "Best pipeline: XGBClassifier(input_matrix, learning_rate=1.0, max_depth=1, min_child_weight=19, n_estimators=100, n_jobs=1, subsample=0.3, verbosity=0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "X,Y = getBFS(part=[0,-1],dataset='sf')\n",
    "X = X.values\n",
    "Ybin = binarize(Y.reshape(-1,1),threshold=40).ravel()\n",
    "\n",
    "from tpot import TPOTClassifier\n",
    "tpot = TPOTClassifier(generations=5, cv=10, scoring='f1', template='Classifier',population_size=100, verbosity=2, n_jobs=30,random_state=42,config_dict=tpotcla_config)\n",
    "tpot.fit(X,Ybin)\n",
    "# print(tpot.score(X,Ybin))\n",
    "bestfullcla = tpot.fitted_pipeline_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ae1b3277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSEfull,RMSEreg,MAPEfull,MAPEreg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/.local/lib/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.0,71.0,205.0,143.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/.local/lib/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.0,68.0,180.0,114.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/.local/lib/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.0,65.0,204.0,126.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/.local/lib/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.0,68.0,250.0,139.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/.local/lib/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.0,72.0,197.0,128.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/.local/lib/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.0,61.0,203.0,124.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/.local/lib/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.0,76.0,226.0,128.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/.local/lib/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.0,66.0,195.0,117.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/.local/lib/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.0,74.0,206.0,132.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigore/.local/lib/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.0,65.0,202.0,120.0\n",
      "RMSEfull,RMSEreg,MAPEfull,MAPEreg\n",
      "77.0,71.0,205.0,143.0\n",
      "71.0,68.0,180.0,114.0\n",
      "66.0,65.0,204.0,126.0\n",
      "73.0,68.0,250.0,139.0\n",
      "75.0,72.0,197.0,128.0\n",
      "65.0,61.0,203.0,124.0\n",
      "79.0,76.0,226.0,128.0\n",
      "72.0,66.0,195.0,117.0\n",
      "78.0,74.0,206.0,132.0\n",
      "67.0,65.0,202.0,120.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.preprocessing import binarize\n",
    "X,Y = getBFS(part=[0,-1],dataset='sf')\n",
    "X = X.values\n",
    "\n",
    "# def quantilefold(X,Y):\n",
    "#     zipped = list(zip(X,Y))\n",
    "#     X = [A for A,B in sorted(zipped, key = lambda x: x[1])]\n",
    "#     Y = [B for A,B in sorted(zipped, key = lambda x: x[1])]\n",
    "#     X = np.array(X)\n",
    "#     Y = np.array(Y)\n",
    "#     return X,Y\n",
    "\n",
    "# X,Y = quantilefold(X,Y)\n",
    "\n",
    "\n",
    "Ybin = binarize(Y.reshape(-1,1),threshold=40).ravel()\n",
    "Yfull = np.concatenate([Y.reshape(-1,1),Ybin.reshape(-1,1)],axis=1)\n",
    "\n",
    "kf = KFold(n_splits=10,shuffle=True)\n",
    "kf.get_n_splits(X,Yfull)\n",
    "\n",
    "print(\"RMSEfull,RMSEreg\")\n",
    "\n",
    "RES=[]\n",
    "\n",
    "for train_index, test_index in kf.split(X,Yfull):\n",
    "    \n",
    "    # reset models (since some may use iterative training)\n",
    "#     bestAreg.fit([[0],[0]],[0,0])\n",
    "#     bestBreg.fit([[0],[0]],[0,0])\n",
    "#     bestfullreg.fit([[0],[0]],[0,0])\n",
    "#     bestfullcla.fit([[0],[0]],[0,1])\n",
    "#     print()\n",
    "    \n",
    "    Xtr, Xte = X[train_index], X[test_index]\n",
    "    Ytr, Yte = Yfull[train_index], Yfull[test_index]\n",
    "    \n",
    "    # one regression model predictions\n",
    "    reg = bestfullreg\n",
    "    reg.fit(Xtr,np.log1p(Ytr[:,0]))\n",
    "    Rpreds = np.expm1(reg.predict(Xte))\n",
    "    RTrpreds = np.expm1(reg.predict(Xtr))\n",
    "    \n",
    "    #one classification model predictions\n",
    "    cla = bestfullcla\n",
    "    cla.fit(Xtr,Ytr[:,1])\n",
    "    Cpreds = cla.predict(Xte)\n",
    "    CTrpreds = cla.predict(Xtr)\n",
    "    \n",
    "    \n",
    "    # A- and B- models try to predict\n",
    "    regA = bestAreg\n",
    "    regB = bestBreg\n",
    "    \n",
    "    As = Ytr[:,1]==0\n",
    "    Bs = Ytr[:,1]==1\n",
    "    regA.fit(Xtr[As],np.log1p(Ytr[As,0]))\n",
    "    regB.fit(Xtr[Bs],np.log1p(Ytr[Bs,0]))\n",
    "    \n",
    "    Apreds = np.expm1(regA.predict(Xte))\n",
    "    Bpreds = np.expm1(regB.predict(Xte))\n",
    "    \n",
    "    ATrpreds = np.expm1(regA.predict(Xtr))\n",
    "    BTrpreds = np.expm1(regB.predict(Xtr))\n",
    "    #accumulating predictions\n",
    "    \n",
    "    R3tr = np.concatenate([reg.predict(Xtr).reshape(-1,1), regA.predict(Xtr).reshape(-1,1), regB.predict(Xtr).reshape(-1,1), cla.predict(Xtr).reshape(-1,1)],axis=1)\n",
    "    R3te = np.concatenate([reg.predict(Xte).reshape(-1,1), regA.predict(Xte).reshape(-1,1), regB.predict(Xte).reshape(-1,1), cla.predict(Xte).reshape(-1,1)],axis=1)\n",
    "#     print(R3tr.shape,Ytr[:,0].shape)\n",
    "\n",
    "    from sklearn.linear_model import RidgeCV\n",
    "    \n",
    "    from sklearn.tree import DecisionTreeRegressor as DTR\n",
    "    full = XGBRegressor()#DTR(max_depth=4)\n",
    "#     full = TPOTRegressor(generations=5, template='Regressor',population_size=40, verbosity=2, n_jobs=20,random_state=42, scoring='neg_mean_squared_error',config_dict=tpotreg_config)\n",
    "    full.fit(R3tr,np.log1p(Ytr[:,0]))\n",
    "#     bestfullreg.fit([[0],[0]],[0,0])\n",
    "#     finalreg = bestfullreg\n",
    "#     finalreg.fit(R3tr,Ytr[:,0])\n",
    "    \n",
    "    \n",
    "    PREDS = np.expm1(full.predict(R3te))\n",
    "    \n",
    "    \n",
    "    \n",
    "    TRPREDS = []\n",
    "    \n",
    "    for i in range(len(Xtr)):\n",
    "        \n",
    "        if CTrpreds[i]==0:\n",
    "            TRPREDS.append(ATrpreds[i])\n",
    "            \n",
    "        if CTrpreds[i]==1:\n",
    "            TRPREDS.append(BTrpreds[i])\n",
    "            \n",
    "    TEPREDS = []\n",
    "    \n",
    "    for i in range(len(Xte)):\n",
    "        \n",
    "        if Cpreds[i]==0:\n",
    "            TEPREDS.append(Apreds[i])\n",
    "            \n",
    "        if Cpreds[i]==1:\n",
    "            TEPREDS.append(Bpreds[i])  \n",
    "    TRPREDS = np.array(TRPREDS).reshape(-1,1)  \n",
    "    \n",
    "#     print(TRPREDS.shape)\n",
    "    TEPREDS = np.array(TEPREDS).reshape(-1,1)\n",
    "#     print(TEPREDS.shape)\n",
    "    \n",
    "    full.fit(np.concatenate([TRPREDS,CTrpreds.reshape(-1,1),ATrpreds.reshape(-1,1),BTrpreds.reshape(-1,1)],axis=1), np.log1p(Ytr[:,0]))\n",
    "    PREDS = full.predict(np.concatenate([TEPREDS,Cpreds.reshape(-1,1),Apreds.reshape(-1,1),Bpreds.reshape(-1,1)],axis=1))\n",
    "    PREDS = np.expm1(PREDS)\n",
    "    PREDS=TEPREDS\n",
    "    \n",
    "    rA, rB = np.sqrt(mse(Yte[:,0],np.array(PREDS))), np.sqrt(mse(Yte[:,0],np.array(Rpreds)))\n",
    "\n",
    "    rA = np.round(rA)\n",
    "    rB = np.round(rB)\n",
    "\n",
    "    print(\"{},{}\".format(rA, rB))\n",
    "    RES.append(\"{},{}\".format(rA, rB))\n",
    "#     print(PREDS[0],Rpreds[0])\n",
    "\n",
    "\n",
    "print(\"RMSEfull,RMSEreg\")\n",
    "print(\"\\n\".join(RES))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
